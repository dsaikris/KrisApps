import streamlit as st
import matplotlib.pyplot as plt
from transformers import GPT2LMHeadModel, GPT2Tokenizer

st.set_page_config(
    page_title="ToyGenAI",
    page_icon="ðŸ‘‹",
)

st.markdown("# ToyGenAI")

model_name = "gpt2"
model = GPT2LMHeadModel.from_pretrained(model_name)
tokenizer = GPT2Tokenizer.from_pretrained(model_name)

col1, col2  = st.columns(2)
with col1:
    st.markdown("## Past some text here")
    text_input = st.text_area("","""
I have no special talents. I am only passionately curious.
""", height=500, max_chars=300)
    if not text_input: text_input=""
    inputs = tokenizer.encode(text_input, return_tensors="pt")
    outputs = model.generate(inputs, max_length=inputs.shape[1] + 50, num_return_sequences=1, no_repeat_ngram_size=4)
with col2:
    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    st.markdown(f"## {model_name.upper()} says! ðŸ‘‹")
    st.text_area("", generated_text + " ..", height=500, disabled=True)

def plot_token_bars(token_ids, tokens, title, dispaly_function):
    fig, ax = plt.subplots()
    bars = ax.bar(range(len(token_ids)), token_ids, tick_label=tokens)
    plt.xticks(rotation=90)
    for bar, token in zip(bars, tokens):
        yval = bar.get_height()
        font_size = max(8, 100 // len(tokens))
        ax.text(bar.get_x() + bar.get_width()/2, yval * 0.2, dispaly_function(token, yval), ha='center', va='bottom', rotation=90, fontsize=font_size)
    ax.set_xticks(range(0, len(tokens), max(1, len(tokens) // 5)))
    ax.set_xticklabels(range(0, len(tokens), max(1, len(tokens) // 5)), rotation=90)
    ax.set_ylabel(title)
    st.pyplot(fig)

def display(token, yval):
    return f"{token} = {yval}"

def displayNone(token, yval):
    return ""

st.markdown("## How did it do?")
st.markdown("""GPT2 does in in 3 main steps.
1. Encode tokens
1. Generate output tokens
1. Decode output tokens.
### Encode tokens
GPT2 transformer will convert input words into tokens and then encode them to a number. Think of it like a dictionary of all words/tokens and the number is its index.
It will use its pre trained transformer neural network to get the indices which I observed are always the same.
Here are the tokens for your text.
""")
tokens = tokenizer.convert_ids_to_tokens(inputs[0])
token_ids = inputs[0].numpy()
st.code(f"Tokens:\n{tokens}")
st.code(f"Tokens as numbers:\n{token_ids}")
plot_token_bars(token_ids, tokens, "Input Tokens", dispaly_function=display)

st.markdown("""### Generate output tokens
Then it will use its large neural network to get the next token number and then the next token number for 50 times. I set is to 50 here you can set a larger one for more generated output. That is it will generate word by word just that it only understands numbers.
""")
output_tokens = tokenizer.convert_ids_to_tokens(outputs[0])
output_token_ids = outputs[0].numpy()
st.code(f"Token numbers generated by GenAi neural network:\n{output_token_ids}")
st.markdown("""### Decode output tokens
Finally, it will convert output token numbers into words/tokens.
""")
plot_token_bars(output_token_ids, output_tokens, "Output Tokens",dispaly_function=displayNone)
st.code(f"Tokens:\n{output_tokens}")
st.text_area("Generated text", generated_text + " ..", height=500, disabled=True)
st.markdown("""## Finally the code that you can take away
""")
st.code("""
inputs = tokenizer.encode(text_input, return_tensors="pt")
outputs = model.generate(
        inputs, max_length=inputs.shape[1] + 50,
        num_return_sequences=1,
        no_repeat_ngram_size=4)
generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
""")
